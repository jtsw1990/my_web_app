{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is always a risk of \"overfitting\" the data when training machine learning algorithms. However, what does overfitting actually mean and how can we, well not do that? In this project we will make use of some simple interactive widgets to give the reader a better understanding of:\n",
    "- What does overfitting look like?\n",
    "- When is it considered overfitting?\n",
    "- How to prevent overfitting?\n",
    "\n",
    "Have a play at the widgets below and pay attention to how to background and bar graphs change!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K Nearest Neighbours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T12:03:06.845567Z",
     "start_time": "2019-03-13T12:03:06.841617Z"
    }
   },
   "outputs": [],
   "source": [
    "# nbi:hide_in\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import svm\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from ipywidgets import interact\n",
    "import ipywidgets as widgets\n",
    "import numpy as np\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T12:14:07.807257Z",
     "start_time": "2019-03-13T12:14:06.887265Z"
    },
    "code_folding": [],
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67dc4c963d2141a48d1b9552b13eae90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(IntSlider(value=1, description='neighbours', max=35, min=1), Checkbox(value=False, descr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# nbi:hide_in\n",
    "@interact(test_data=widgets.Checkbox(value=False, description=\"Show test data only\",  disabled=False), \n",
    "          neighbours=widgets.IntSlider(min=1, max=35, step=1))\n",
    "def plot_decision_boundaries(neighbours, test_data):\n",
    "    MESH_STEP_SIZE = 0.01\n",
    "    iris = load_iris()\n",
    "    X, y = iris.data[:, :2], iris.target\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    clf = KNeighborsClassifier(neighbours)\n",
    "    clf.fit(X_train, y_train)\n",
    "    min_x, max_x = X_train[:, 0].min() - 1.0, X_train[:, 0].max() + 1.0\n",
    "    min_y, max_y = X_train[:, 1].min() - 1.0, X_train[:, 1].max() + 1.0\n",
    "    x_vals, y_vals = np.meshgrid(\n",
    "    np.arange(min_x, max_x, MESH_STEP_SIZE), np.arange(\n",
    "                min_y, max_y, MESH_STEP_SIZE)\n",
    "        )\n",
    "    output = clf.predict(np.c_[x_vals.ravel(), y_vals.ravel()])\n",
    "    output = output.reshape(x_vals.shape)\n",
    "    x = np.arange(0, 10, 0.2)\n",
    "    y = np.sin(x)\n",
    "    fig = plt.figure(figsize=(10, 12)) \n",
    "    gs = gridspec.GridSpec(2, 1, height_ratios=[3, 1]) \n",
    "    ax0 = plt.subplot(gs[0])\n",
    "    ax0.pcolormesh(x_vals, y_vals, output, cmap=plt.cm.Pastel1)\n",
    "    if test_data:\n",
    "        ax0.scatter(\n",
    "                X_test[:, 0], X_test[:, 1], c=y_test, s=20, edgecolors=\"black\", linewidth=1, cmap=plt.cm.Set1\n",
    "            )\n",
    "    else:\n",
    "        ax0.scatter(\n",
    "                X_train[:, 0], X_train[:, 1], c=y_train, s=20, edgecolors=\"black\", linewidth=1, cmap=plt.cm.Set1\n",
    "            )        \n",
    "    ax0.set_xlim(x_vals.min(), x_vals.max())\n",
    "    ax0.set_ylim(y_vals.min(), y_vals.max())\n",
    "    ax1 = plt.subplot(gs[1])\n",
    "    scores = [clf.score(X_train, y_train), clf.score(X_test, y_test)]\n",
    "    names = (\"Train\", \"Test\")\n",
    "    y_pos = np.arange(len(names))\n",
    "    ax1.barh(y_pos, scores, align=\"center\", alpha=0.5)\n",
    "    ax1.set_xticks(y_pos, names)\n",
    "    ax1.set_yticks(y_pos)\n",
    "    ax1.set_yticklabels(names)\n",
    "    ax1.invert_yaxis()\n",
    "    ax1.set_xlabel('Accuracy')\n",
    "    totals = []\n",
    "    for i in ax1.patches:\n",
    "        totals.append(i.get_width())\n",
    "    total = sum(totals)\n",
    "    for i in ax1.patches:\n",
    "        ax1.text(i.get_width()-.1, i.get_y()+.43, \\\n",
    "            str(round((i.get_width())*100, 2))+'%', fontsize=10,\n",
    "            color='black')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The slider above controls a hyperparameter specific to the Nearest Neighbour algorithm. It basically tells the algorithm how many points to look at in order to decide if a particular point belongs to a category. As you would have guessed, a neighbour=1 model would mean that any data point would just refer to its closest neighbour and take its category. We can see from the different background colours, which represent the decision boundary made by the algorithm that each number of neighbours lead to a very specific set of boundaries. As we increase the number of neighbours, we can observe how the model actually becomes less prone to overfitting, but at the same time miss out some inherent patterns between the different categories. Can you tell which setting of neighbours give the best overall training and testing accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support vector machines, or SVMs are another popular classification algorthim. Note that for multi-class classification, a OVR (one vs rest) approach is taken here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T12:20:36.966005Z",
     "start_time": "2019-03-13T12:20:35.895152Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b05ddf8256e496d9eda11f7e5ed6931",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "interactive(children=(FloatSlider(value=0.1, description='reg', max=10.0, min=0.1), ToggleButtons(description=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# nbi:hide_in\n",
    "@interact(\n",
    "    test_data=widgets.Checkbox(value=False, description=\"Show test data only\",  disabled=False), \n",
    "    kernel=widgets.ToggleButtons(\n",
    "    options=[\"linear\", \"rbf\", \"poly\"],\n",
    "    description=\"Kernel selection\",\n",
    "    disabled=False\n",
    "),\n",
    "    reg=widgets.FloatSlider(min=0.1, max=10, step=0.1)\n",
    "          )\n",
    "def plot_decision_boundaries(reg, kernel, test_data):\n",
    "    MESH_STEP_SIZE = 0.01\n",
    "    iris = load_iris()\n",
    "    X, y = iris.data[:, :2], iris.target\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    clf = svm.SVC(C=reg, kernel=kernel, gamma=\"scale\", random_state=42)\n",
    "    \n",
    "    clf.fit(X_train, y_train)\n",
    "    min_x, max_x = X_train[:, 0].min() - 1.0, X_train[:, 0].max() + 1.0\n",
    "    min_y, max_y = X_train[:, 1].min() - 1.0, X_train[:, 1].max() + 1.0\n",
    "    x_vals, y_vals = np.meshgrid(\n",
    "    np.arange(min_x, max_x, MESH_STEP_SIZE), np.arange(\n",
    "                min_y, max_y, MESH_STEP_SIZE)\n",
    "        )\n",
    "    output = clf.predict(np.c_[x_vals.ravel(), y_vals.ravel()])\n",
    "    output = output.reshape(x_vals.shape)\n",
    "    x = np.arange(0, 10, 0.2)\n",
    "    y = np.sin(x)\n",
    "    fig = plt.figure(figsize=(10, 12)) \n",
    "    gs = gridspec.GridSpec(2, 1, height_ratios=[3, 1]) \n",
    "    ax0 = plt.subplot(gs[0])\n",
    "    ax0.pcolormesh(x_vals, y_vals, output, cmap=plt.cm.Pastel1)\n",
    "    if test_data:\n",
    "        ax0.scatter(\n",
    "                X_test[:, 0], X_test[:, 1], c=y_test, s=20, edgecolors=\"black\", linewidth=1, cmap=plt.cm.Set1\n",
    "            )\n",
    "    else:\n",
    "        ax0.scatter(\n",
    "                X_train[:, 0], X_train[:, 1], c=y_train, s=20, edgecolors=\"black\", linewidth=1, cmap=plt.cm.Set1\n",
    "            )        \n",
    "    ax0.set_xlim(x_vals.min(), x_vals.max())\n",
    "    ax0.set_ylim(y_vals.min(), y_vals.max())\n",
    "    ax1 = plt.subplot(gs[1])\n",
    "    scores = [clf.score(X_train, y_train), clf.score(X_test, y_test)]\n",
    "    names = (\"Train\", \"Test\")\n",
    "    y_pos = np.arange(len(names))\n",
    "    ax1.barh(y_pos, scores, align=\"center\", alpha=0.5)\n",
    "    ax1.set_xticks(y_pos, names)\n",
    "    ax1.set_yticks(y_pos)\n",
    "    ax1.set_yticklabels(names)\n",
    "    ax1.invert_yaxis()\n",
    "    ax1.set_xlabel('Accuracy')\n",
    "    totals = []\n",
    "    for i in ax1.patches:\n",
    "        totals.append(i.get_width())\n",
    "    total = sum(totals)\n",
    "    for i in ax1.patches:\n",
    "        ax1.text(i.get_width()-.1, i.get_y()+.43, \\\n",
    "            str(round((i.get_width())*100, 2))+'%', fontsize=10,\n",
    "            color='black')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reg corresponds to C in the algorithim and it tells the SVM optimization how much you want to avoid misclassifying each training example. For large values of C, the optimization will choose a smaller-margin hyperplane if that hyperplane does a better job of getting all the training points classified correctly. Conversely, a very small value of C will cause the optimizer to look for a larger-margin separating hyperplane, even if that hyperplane misclassifies more points. For very tiny values of C, you should get misclassified examples, often even if your training data is linearly separable. If you want a deeper dive into the mechanics of SVMs, there are some pretty awesome links in the [resources](https://whobrokemycode.netlify.com/resources/) page."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
